---
title: "Perceptron & Generalized Linear Model"
date: "2025-01-01 18:21:00"
categories: 
    - Machine Learning
    - CS229
tags: 
    - Machine Learning
    - Stanford
    - CS229
mathjax: true
---
# Perceptron

Perceptron is a simple algorithm that is used to classify data into two categories. It is a hyperplane that separates the data into two categories. In other words, it is a hard version of logistic regression defined as:

$$
h_{\theta}(x) = g(\theta^T x)
$$

$$
g(z) = \begin{cases}
1 & \text{if } z \geq 0 \\
0 & \text{otherwise}
\end{cases}
$$

If there is a new data point, which cannot be classified by the linear classifier, the perceptron will update its weights to classify the new data point by adjusting the parameter $\theta$.

$$
\theta_{j} := \theta_{j} + \alpha(y^{(i)} - h_{\theta}(x^{(i)}))x_{j}^{(i)}
$$

- **Perceptron algorithm** updates when there is a misclassification. 

- **Logistic regression** updates by minimizing the cost function. 

# Exponential Familys

**Probability Density Function (PDF)** of the exponential family is defined as:

$$
p(y;\eta) = b(y)exp(\eta^T T(y) - a(\eta))
$$

- $y$ is the data

- $\eta$ is the natural parameter

- $T(y)$ is the sufficient statistic

- $b(y)$ is the base measure

- $a(\eta)$ is the log-partition function

We write **Bernoulli distribution** as an example of the exponential family:

$$
\begin{aligned}
p(y;\phi) &= \phi^y(1-\phi)^{1-y} \\
&= exp(ylog(\phi) + (1-y)log(1-\phi)) \\
&= exp((log(\frac{\phi}{1-\phi}))y + log(1-\phi)) \\
\end{aligned}
$$

- $b(y) = 1$

- $\eta = log(\frac{\phi}{1-\phi}) \implies \phi = \frac{1}{1+e^{-\eta}}$

- $T(y) = y$

- $a(\eta) = -log(1-\phi)$

For **Gaussian distribution**:

$$
\begin{aligned}
p(y;\mu) &= \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(y-\mu)^2}{2\sigma^2}) \\
&= \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-y^2}{2}}exp(\frac{y\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2}) \\
\end{aligned}
$$

- $b(y) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-y^2}{2}}$

- $T(y) = y$

- $\eta = \frac{\mu}{\sigma^2} \implies \mu = \eta\sigma^2$

- $a(\eta) = \frac{\mu^2}{2\sigma^2}$

## Properties of Exponential Family

1. **MLE** w.r.t. $\eta \implies$ **Concave**  

2. **MSE** w.r.t. $\eta \implies$ **Convex**

3. $E[y; \eta] = \frac{\partial}{\partial \eta}a(\eta)$

4. $Var[y; \eta] = \frac{\partial^2}{\partial \eta^2}a(\eta)$

## General Form of Exponential Family

- $\mathbb{R} \to$ Gaussian

- $\{0, 1\} \to$ Bernoulli

- $\mathbb{N} \to$ Poisson

- $\mathbb{R}^{+} \to$ Gamma, Exponential

- $Distribution \to$ Beta, Dirichlet

# Generalized Linear Models (GLMs)

## Assumptions / Design Choices:

1. $y \vert x \sim ExponentialFamily(\eta)$

2. $\eta = \theta^T x$, where $\theta \in \mathbb{R}^{n}, x \in \mathbb{R}^{n}$

3. Test time: Output $h_{\theta}(x) = E[y \vert x; \theta]$

## Training

Learning Update Rule:

1. SGD: 

$$
\theta := \theta + \alpha(y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}
$$

2. Batch GD:

$$
\theta := \theta + \alpha\sum_{i=1}^{m}(y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}
$$

## Term

Given that $g(\eta) = \frac{\partial}{\partial \eta}a(\eta)$

- $\eta \to$ natural parameter

- $E[y \vert x; \theta] = \mu = g(\eta) \to$ Canonical Response function

- $\eta = g^{-1}(\mu) \to$ Canonical Link function

## Examples

**Linear Regression**: $h_{\theta}(x) = E[y \vert x; \theta] = \phi = \frac{1}{1+e^{-\theta^T x}}$

# Softmax Regression (Multiclass Classification)

Assume that we have a dataset with multible classes, and need to do a classification. Given that the number of classes is $3$, $x^{(i)} \in \mathbb{R}^{n}$, $y^{(i)} \in \{1,2,3\}$, $\theta_{j \in \{1,2,3\}} \in \mathbb{R}^{n}$. 

<span style="text-decoration: underline;">**Claim:**</span> This multinomial model is a member of the exponential family

<span style="text-decoration: underline;">**Proof:**</span> Let $P(y^{(i)} = j) = \phi_{j}$

$$
\begin{aligned}
P(y^{(i)}; \phi) &= \phi_{1}^{\mathbb{I}_{1}(y^{(i)})}\phi_{2}^{\mathbb{I}_{2}(y^{(i)})}\phi_{3}^{\mathbb{I}_{3}(y^{(i)})} \\
&= \exp\left(\sum_{j=1}^{3} \mathbb{I}_{j}(y^{(i)}) \log(\phi_{j})\right) \\
&= \exp\left(\mathbb{I}_{1}(y^{(i)}) \log\left(\frac{\phi_{1}}{\phi_{3}}\right) + \mathbb{I}_{2}(y^{(i)}) \log\left(\frac{\phi_{2}}{\phi_{3}}\right) + \log(\phi_{3})\right) \\
\end{aligned}
$$

where

$$
\begin{aligned}
\eta &= 
\begin{bmatrix}
log(\phi_{1}/{\phi_{3}}) \\
log(\phi_{2}/{\phi_{3}}) \\
\end{bmatrix} \\
a(\eta) &= -log(\phi_{3}) \\
b(y^{(i)}) &= 1 \\
\end{aligned}
$$

Therefore, we can have 

$$
\begin{aligned}
e^{\eta_{j}} &= \log(\frac{\phi_{j}}{\phi_{3}}) \\
\phi_{3}e^{\eta_{j}} &= \phi_{j} \\
\phi_{3}\sum_{k=1}^{3}e^{\eta_{k}} &= \sum_{k=1}^{3}\phi_{k} = 1 \\
\phi_{j} &= \frac{e^{\eta_{j}}}{\sum_{k=1}^{3}e^{\eta_{j}}}
\end{aligned}
$$

$\theta^{T}x$ create three hyperplanes to classify the data. By substituting the hyperplanes into the logistic function separately, we can get the probability of each class.

$$
P(y^{(i)} = j \vert x^{(i)}; \theta) = \frac{exp(\theta_{j}^{T}x^{(i)})}{\sum_{k=1}^{3}exp(\theta_{k}^{T}x^{(i)})}
$$

By Minimize the **Cross-Entropy:**

$$
\begin{aligned}
CE(\mathbb{I_{j}},P) &= -\sum_{y \in {1,2,3}}\mathbb{I_{j}(y)}log(P(y^{(i)} = j \vert x^{(i)}; \theta)) \\
&= -log(P(y^{(i)} = j \vert x^{(i)}; \theta)) \\
&= -log(\frac{exp(\theta_{j}^{T}x^{(i)})}{\sum_{k=1}^{3}exp(\theta_{k}^{T}x^{(i)})}) \\
\end{aligned}
$$

Finally, find the value by Gradient Descend

**TIPS:** Cross-Entropy is a commonly used loss function for classification tasks, especially in neural networks. It measures the difference between the true label distribution and the predicted label distribution. The goal is to minimize this difference, thereby improving the accuracy of the model.