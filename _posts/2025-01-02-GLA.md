---
title: "Generative Learning Algorithms"
date: "2025-01-02 13:54:00"
categories: 
    - Machine Learning
    - CS229
tags: 
    - Machine Learning
    - Stanford
    - CS229
mathjax: true
---
# Generative & Discriminative Comparison

**Discriminative algorithms** learn $P(y \vert x)$ directly or learn a direct map from inputs $x$ to outputs $y$.

- Choose parameters $\theta$ to maximize $P(y \vert x; \theta)$

- Weaker assumptions (Cannot implies Generative)

**Generative algorithms** learn $P(x \vert y)$ and $P(y)$, and then use Bayes rule to compute $P(y \vert x)$, where $x$ is the feature, and $y$ is the class.

- Choose parameters $\theta$ to maximize $P(x, y)$

- Stronger assumptions (Can implies Logistic Regression)

**TIPS:** 

**Conditional Probability** is defined as:

$$
P(x, y) = P(x \vert y)P(y) = P(y \vert x)P(x)
$$

**Bayes Rule** is defined as:

$$
\begin{aligned}
P(y \vert x) &= \frac{P(x \vert y)P(y)}{P(x)} \\
P(x) &= \sum_{y} P(x \vert y)P(y)
\end{aligned}
$$

- $P(y)$ is the prior probability of class y.

- $P(x \vert y)$ is the likelihood of the data given the class.

- $P(x)$ is the evidence.

# Gaussian Discriminant Analysis (GDA)

Suppose $x \in \mathbb{R}^n$ (drop $x_0 = 1$ by convention).

Assume $P(x \vert y)$ is Gaussian:

$$
z \sim \mathcal{N}(\vec{\mu}, \Sigma)
$$

where $z \in \mathbb{R}^n, \vec{\mu} \in \mathbb{R}^n, \Sigma \in \mathbb{R}^{n \times n}$

Then we have:

- $E[z] = \vec{\mu}$

- $Cov[z] = E[(z - \vec{\mu})(z - \vec{\mu})^T] = E[zz^T] - \vec{\mu}\vec{\mu}^T$

- $P(z) = \frac{1}{(2\pi)^{n/2} \vert \Sigma \vert^{1/2}} exp(-\frac{1}{2}(z - \vec{\mu})^T \Sigma^{-1} (z - \vec{\mu}))$

implies:

$$
P(x \vert y = 0) = \frac{1}{(2\pi)^{n/2} \vert \Sigma \vert^{1/2}} exp(-\frac{1}{2}(x - \vec{\mu}_0)^T \Sigma^{-1} (x - \vec{\mu}_0)) \\
P(x \vert y = 1) = \frac{1}{(2\pi)^{n/2} \vert \Sigma \vert^{1/2}} exp(-\frac{1}{2}(x - \vec{\mu}_1)^T \Sigma^{-1} (x - \vec{\mu}_1))
$$

We know that $y \in \{0,1\} \implies y \sim Bernoulli(\phi)$, where $\phi = P(y = 1)$.

$$P(y) = \phi^y(1 - \phi)^{1 - y}$$

Given the training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^m$, in order to fit the parameters $\vec{\mu}_0, \vec{\mu}_1, \Sigma, \phi$, we need to maximize the joint likelihood:

$$
\begin{aligned}
L(\vec{\mu}_0, \vec{\mu}_1, \Sigma, \phi) &= \prod_{i=1}^m P(x^{(i)}, y^{(i)}) \\
&= \prod_{i=1}^m P(x^{(i)} \vert y^{(i)})P(y^{(i)}) \\
\end{aligned}
$$

By MLE, we have:

$$
\begin{aligned}
\vec{\mu}_0 &= \frac{\sum_{i=1}^m \mathbb{I}\{y^{(i)} = 0\}x^{(i)}}{\sum_{i=1}^m \mathbb{I}\{y^{(i)} = 0\}} \\
\vec{\mu}_1 &= \frac{\sum_{i=1}^m \mathbb{I}\{y^{(i)} = 1\}x^{(i)}}{\sum_{i=1}^m \mathbb{I}\{y^{(i)} = 1\}} \\
\phi &= \frac{1}{m} \sum_{i=1}^m \mathbb{I}\{y^{(i)} = 1\} \\
\Sigma &= \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \vec{\mu}_{y^{(i)}})(x^{(i)} - \vec{\mu}_{y^{(i)}})^T
\end{aligned}
$$

The prediction is:

$$\arg \max_y P(y \vert x) = \arg \max_y P(x \vert y)P(y)$$

# Naive Bayes

Natural Language Processing (NLP) is a typical application of Naive Bayes. We need to map the text to a feature vector $x$.

Suppose we have a dictionary of $n$ words, and in the feature vector $x \in \{0,1\}^{n}$, if the word $i$ appears in the text, then $x_i = 1$, otherwise $x_i = 0$.

Assume that $x_{i}$ are conditionally independent given $y$:

$$P(x \vert y) = \prod_{i=1}^n P(x_i \vert y)$$

The parameters are:

$$
\begin{aligned}
\phi_{i \vert y = 1} &= P(x_i = 1 \vert y = 1) \\
\phi_{i \vert y = 0} &= P(x_i = 1 \vert y = 0) \\
\phi_y &= P(y = 1)
\end{aligned}
$$

The joint likelihood is:

$$
L(\phi_{y}, \phi_{j \vert y}) = \prod_{i=1}^m P(x^{(i)}, y^{(i)}; \phi_y, \phi_{j \vert y})
$$

Take MLE, we have:

$$
\begin{aligned}
\phi_{y} &= \frac{1}{m} \sum_{i=1}^m \mathbb{I}\{y^{(i)} = 1\} \\
\phi_{i \vert y = 1} &= \frac{\sum_{i=1}^m \mathbb{I}\{x_i^{(i)} = 1, y^{(i)} = 1\}}{\sum_{i=1}^m \mathbb{I}\{y^{(i)} = 1\}} \\
\end{aligned}
$$

The major problem of Naive Bayes is that if the word does not appear in the training set, then the probability is zero. We can use **Laplace Smoothing** to solve this problem.