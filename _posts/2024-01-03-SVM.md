---
title: "Support Vector Machines & Kernels"
date: "2025-01-03 13:11:00"
categories: 
    - Machine Learning
    - CS229
tags: 
    - Machine Learning
    - Stanford
    - CS229
mathjax: true
---
# Support Vector Machines

Support Vector Machine (SVM) is a algorithm to find potentially non-linear decision boundaries. It maps the feature vector into a higher dimensional space and then apply a linear classifier in that space.

Consider the Logistic Regression model, to generate a linear classifier, we hope that

- $\text{If } y=1, \text{then } \theta^{T}x^{(i)} \gg 0$.

- $\text{If } y=0, \text{then } \theta^{T}x^{(i)} \ll 0$.

In SVM, we set $y \in \{-1, +1\}$ and have $h_{\theta}=g$ output value in $\{-1, +1\}$ s.t.

$$
g(z) = 
\begin{cases}
1 & \text{if } z \geq 0 \\
-1 & \text{otherwise}
\end{cases}
$$

**TIPS:** $h_{\theta}(x) = h_{w,b}(x)$, where $\theta = [b, w]^{T}, b=\theta_{0}, w = [\theta_{1}, \cdots ,\theta_{n}]^{T}$.

Functional margin of hyperplane defined by $(w, b)$ with respect to $(x^{(i)}, y^{(i)})$ is:

$$
\hat{\gamma}^{(i)} = y^{(i)}(w^{T}x^{(i)} + b)
$$

- $\text{If } y^{(i)} = 1, \text{want } w^{T}x^{(i)} + b \gg 1$.
- $\text{If } y^{(i)} = -1, \text{want } w^{T}x^{(i)} + b \ll -1$.

Functional margin w.r.t. training set is:

$$
\hat{\gamma} = \min_{i=1,\cdots, m} \hat{\gamma}^{(i)}
$$

The Geometric margin of hyperplane is:

$$
\begin{aligned}
\gamma^{(i)} &= y^{(i)}(\frac{w^{T}x^{(i)} + b}{\Vert w \Vert}) \\
\gamma &= \min_{i=1,\cdots, m} \gamma^{(i)}
\end{aligned} 
$$

Therefore, the optimal margin classifier is to choose $(w, b)$ to maximize $\gamma$ s.t. $y^{(i)}(\frac{w^{T}x^{(i)} + b}{\Vert w \Vert}) \geq \gamma, i=1,\cdots, m$.

By reformulating the problem, we have the following optimization problem:

$$
\max_{\gamma, w, b} {\Vert w \Vert}^{2} \\
\text{s.t. } y^{(i)}({w^{T}x^{(i)} + b}) \geq 1, i=1,\cdots, m
$$