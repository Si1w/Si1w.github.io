---
title: Natural Language Processing with Deep Learning
date: 2026-01-10
categories: [NLP, Deep Learning]
mermaid: true
---

# Contents

- [Word Vectors](#word-vectors)
  - [One Hot Vectors](#one-hot-vectors)
  - [Word2Vec](#word2vec)

# Word Vectors

## One Hot Vectors

One hot vectors are a simple way to represent words as vectors. Each word in the vocabulary is represented by a vector of length equal to the size of the vocabulary, with a `1` in the position corresponding to the word and `0` elsewhere.

For example, if our vocabulary consists of the words ["cat", "dog", "people"], the one hot vectors would be:

$$
\begin{align*}
\text{cat} &= [1, 0, 0] \\
\text{dog} &= [0, 1, 0] \\
\text{people} &= [0, 0, 1]
\end{align*}
$$

In this way, the vector dimension is equal to the size of the vocabulary. Every vector is orthogonal to each other, which means one hot vectors do not capture any natural, inherent sense of the meaning of words.

> "You shall know a word by the company it keeps." - J.R. Firth

## Word2Vec

The [Word2vec](https://arxiv.org/pdf/1301.3781) framework operates on the supposition that we have a large corpus of text (a long list of words) where every word in a fixed vocabulary is represented by a **vector**. The process functions by going through each position $t$ in the text, identifying a centre word $c$ and its corresponding context words $o$. By using the **similarity of the word vectors** for $c$ and $o$, the model can **calculate the probability** of $o$ given $c$. To refine the model, we **Keep adjusting the word vectors** in an iterative process to **maximize the probability** of the actual context words given the centre word.

<div style="text-align:center;">
  <div class="mermaid" style="display:inline-block;">
graph TD
    playing -->|"P(w<sub>t-2</sub> | w<sub>t</sub>)"| Connor
    playing -->|"P(w<sub>t-1</sub> | w<sub>t</sub>)"| likes
    playing -->|"P(w<sub>t+1</sub> | w<sub>t</sub>)"| computer
    playing -->|"P(w<sub>t+2</sub> | w<sub>t</sub>)"| games
  </div>
</div>

Here, the centre word is "playing" and the context words are "Connor", "likes", "computer", and "games"

For each position $t=1, 2, \dots, T$, predict context words within a window of fixed size $m$, given centre word $w_t$, the datalikelihood function is defined as:

$$
L(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m \atop {j \neq 0}} P(w_{t+j} | w_t; \theta)
$$

The objective function $J(\theta)$ is the average negative log likelihood of the data:

$$
J(\theta) = - \frac{1}{T} log L(\theta) = - \frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop {j \neq 0}} log P(w_{t+j} | w_t; \theta)
$$

To calculate the conditional probability $P(w_{t+j} \vert w_t; \theta)$, we will use two vectors to represent each word in the vocabulary:$v_w$ when $w$ is the centre word, and $u_w$ when $w$ is a context word. The conditional probability is defined using the softmax function:

$$
P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w \in V} exp(u_w^T v_c)}
$$




